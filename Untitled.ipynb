{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from gensim.models.wrappers.ldamallet import malletmodel2ldamodel\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pyLDAvis.gensim\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "from preprocessor import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conectando com o MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client[\"discursoDB\"]\n",
    "discursos = db[\"discursos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discursos preprocessados, demorou 00:06:01:01\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# all_discursos = discursos.find()\n",
    "# discursos_list = random.sample(list(all_discursos), 5000)\n",
    "\n",
    "discursos_list = discursos.find({\"SiglaPartidoParlamentarNaData\": {\"$eq\": \"PT\"}})\n",
    "\n",
    "text = []\n",
    "\n",
    "for disc in discursos_list:\n",
    "    discurso_text = disc[\"Conteudo\"]\n",
    "    \n",
    "    if discurso_text:\n",
    "        text.append(discurso_text)\n",
    "\n",
    "pipe = Pipeline( [\n",
    "    ('cleaning', Cleaner()), \n",
    "    ('stopwords', StopWords(lang='portuguese', tokenize=True)),\n",
    "    ('stemming', Stemmer(lang='portuguese', fit_reuse=True))])\n",
    "pipe.fit(text)\n",
    "res = pipe.transform(text)\n",
    "\n",
    "# preproc = Preprocessor(max_word_lenght=2)\n",
    "# preproc.fit(text)\n",
    "# text = preproc.transform(text)        \n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(time.strftime(\"Discursos preprocessados, demorou %H:%M:%S:%m\", time.gmtime(elapsed_time)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do modelo LDA com Mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lda model criado, demorou 00:15:37:01\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Create a corpus from a list of texts\n",
    "data = [a.split() for a in res]\n",
    "\n",
    "dictionary = Dictionary(data)\n",
    "\n",
    "corpus = [dictionary.doc2bow(t) for t in data]\n",
    "\n",
    "# os.environ['MALLET_HOME'] = 'X:\\\\Programs\\\\mallet\\\\mallet-2.0.8\\\\'\n",
    "mallet_path = 'X:\\\\Programs\\\\mallet\\\\mallet-2.0.8\\\\bin\\\\mallet.bat'\n",
    "\n",
    "# Train the model on the corpus.\n",
    "lda = LdaMallet(mallet_path, corpus, id2word=dictionary, num_topics=10)\n",
    "# lda = LdaModel(corpus, id2word=dictionary, num_topics=10, alpha='auto', eval_every=5, chunksize=10, passes=10, decay=0.9)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(time.strftime(\"Lda model criado, demorou %H:%M:%S:%m\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions nos stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: ['senador', 'nao', 'exa', 'presidente', 'blocopt', 'todos', 'parte', 'porque', 'eduardo', 'ser', 'fazer', 'governo', 'quero', 'dizer', 'pode']\n",
      "Topic: 1 \n",
      "Words: ['todos', 'brasil', 'brasileiro', 'povo', 'presidente', 'vida', 'historia', 'pais', 'grande', 'anos', 'humanos', 'nao', 'mundo', 'dia', 'ser']\n",
      "Topic: 2 \n",
      "Words: ['economia', 'governo', 'nao', 'pais', 'brasil', 'brasileiro', 'empresas', 'bilhoes', 'banco', 'aumento', 'politica', 'investimentos', 'ano', 'emprego', 'medida']\n",
      "Topic: 3 \n",
      "Words: ['publica', 'federal', 'presidente', 'lei', 'pode', 'nao', 'casa', 'ser', 'nacional', 'estado', 'senado', 'justica', 'comissao', 'projeto', 'constituicao']\n",
      "Topic: 4 \n",
      "Words: ['mulheres', 'educacao', 'todos', 'direitos', 'politica', 'trabalho', 'pais', 'pessoas', 'escola', 'sociedade', 'nacional', 'universidade', 'brasileiro', 'violencia', 'tambem']\n",
      "Topic: 5 \n",
      "Words: ['trabalho', 'nao', 'senador', 'paim', 'presidente', 'aqui', 'todos', 'paulo', 'dizer', 'tambem', 'quero', 'grande', 'projeto', 'salario', 'previdencia']\n",
      "Topic: 6 \n",
      "Words: ['nao', 'aqui', 'bloco', 'porque', 'presidente', 'quero', 'senador', 'fazer', 'dizer', 'parlamentar', 'falar', 'vai', 'pais', 'governo', 'pode']\n",
      "Topic: 7 \n",
      "Words: ['senador', 'estado', 'governo', 'aqui', 'todos', 'tambem', 'quero', 'fazer', 'brasil', 'presidente', 'viana', 'acre', 'rio', 'hoje', 'entao']\n",
      "Topic: 8 \n",
      "Words: ['saude', 'programa', 'familia', 'renda', 'pessoas', 'sao', 'todos', 'populacao', 'medicos', 'brasileiro', 'milhoes', 'brasil', 'mil', 'atendimento', 'anos']\n",
      "Topic: 9 \n",
      "Words: ['estado', 'amazonia', 'nao', 'desenvolvimento', 'regiao', 'presidente', 'grande', 'terra', 'todos', 'energia', 'meio', 'sao', 'materia', 'ser', 'brasil']\n"
     ]
    }
   ],
   "source": [
    "for index, topic in lda.show_topics(formatted=False, num_words= 15):\n",
    "    print('Topic: {} \\nWords: {}'.format(index, [pipe.predict([w[0]])[0][0] for w in topic]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: ['estad', 'govern', 'municipi', 'federal', 'tod', 'regia', 'president', 'tamb', 'rio', 'cidad', 'amazon', 'sao', 'senador', 'grand', 'recurs', 'mat', 'projet', 'dess', 'nort', 'faz']\n",
      "Topic: 1 \n",
      "Words: ['nao', 'president', 'public', 'pod', 'govern', 'ministr', 'federal', 'polic', 'fat', 'ser', 'contr', 'crim', 'justic', 'sao', 'cpi', 'dev', 'diz', 'dess', 'ministeri', 'denunc']\n",
      "Topic: 2 \n",
      "Words: ['nao', 'polit', 'part', 'pod', 'ser', 'pais', 'govern', 'brasileir', 'tod', 'reform', 'president', 'dev', 'moment', 'sociedad', 'republ', 'estad', 'nacional', 'outr', 'debat', 'process']\n",
      "Topic: 3 \n",
      "Words: ['nao', 'aqu', 'senador', 'quer', 'porqu', 'faz', 'president', 'diz', 'fal', 'bloc', 'vai', 'enta', 'tod', 'brasil', 'tamb', 'trabalh', 'hoj', 'pod', 'gent', 'agor']\n",
      "Topic: 4 \n",
      "Words: ['govern', 'nao', 'econom', 'banc', 'ano', 'bilho', 'brasileir', 'public', 'aument', 'recurs', 'president', 'trabalh', 'empreg', 'pag', 'brasil', 'med', 'pod', 'salari', 'pais', 'minim']\n",
      "Topic: 5 \n",
      "Words: ['trabalh', 'saud', 'educaca', 'tod', 'public', 'sao', 'pesso', 'escol', 'crianc', 'pais', 'mulh', 'anos', 'direit', 'ser', 'social', 'univers', 'medic', 'sociedad', 'brasileir', 'tamb']\n",
      "Topic: 6 \n",
      "Words: ['pais', 'brasil', 'brasileir', 'econom', 'desenvolv', 'produt', 'produca', 'empres', 'setor', 'grand', 'energ', 'mund', 'agricultur', 'import', 'nao', 'president', 'ser', 'merc', 'maior', 'mei']\n",
      "Topic: 7 \n",
      "Words: ['senador', 'president', 'sen', 'projet', 'federal', 'comissa', 'lei', 'cas', 'mat', 'cam', 'deput', 'aprov', 'nacional', 'congress', 'srs', 'public', 'tamb', 'med', 'apresent', 'art']\n",
      "Topic: 8 \n",
      "Words: ['tod', 'brasil', 'grand', 'vid', 'brasileir', 'anos', 'histor', 'president', 'tamb', 'pov', 'hoj', 'lut', 'dia', 'homenag', 'mund', 'sempr', 'hom', 'human', 'rio', 'livr']\n",
      "Topic: 9 \n",
      "Words: ['nao', 'exa', 'senador', 'porqu', 'quer', 'diz', 'govern', 'president', 'faz', 'aqu', 'tod', 'pmdb', 'cas', 'diss', 'ser', 'pod', 'fal', 'enta', 'sab', 'psdb']\n"
     ]
    }
   ],
   "source": [
    "for index, topic in lda.show_topics(formatted=False, num_words= 20):\n",
    "    print('Topic: {} \\nWords: {}'.format(index, [w[0] for w in topic]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listar tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "# vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\Programs\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:223: RuntimeWarning: divide by zero encountered in log\n",
      "  kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))\n",
      "X:\\Programs\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:240: RuntimeWarning: divide by zero encountered in log\n",
      "  log_lift = np.log(topic_term_dists / term_proportion)\n",
      "X:\\Programs\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:241: RuntimeWarning: divide by zero encountered in log\n",
      "  log_ttd = np.log(topic_term_dists)\n",
      "X:\\Programs\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "model = malletmodel2ldamodel(lda)\n",
    "vis = pyLDAvis.gensim.prepare(model, corpus, dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis\n",
    "pyLDAvis.save_html(vis, 'lda-PT.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-4f37f5dd569e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_topic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"off\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Topic #\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mX:\\Programs\\Anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mfit_words\u001b[1;34m(self, frequencies)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \"\"\"\n\u001b[1;32m--> 361\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_font_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: C901\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mX:\\Programs\\Anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \"\"\"\n\u001b[0;32m    379\u001b[0m         \u001b[1;31m# make sure frequencies are sorted and normalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "for t in range(lda.num_topics):\n",
    "    plt.figure()\n",
    "    plt.imshow(WordCloud().fit_words(lda.show_topic(t, 200)))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Topic #\" + str(t))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
